\documentclass[12pt, titlepage]{article}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{mathtools}
\usepackage{pythonhighlight}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{xcolor}

% algorithm2e
\RestyleAlgo{ruled}
\SetKwProg{Fn}{Function}{}{end}
\SetKw{Stop}{stop}

% titling
\title{\textit{Data Structures \& Algorithms} Cheat Sheet}
\setlength{\droptitle}{-20ex}
\author{Thomas Monson}
\renewcommand\maketitlehookb{\vspace{-3ex}}
\date{}
\renewcommand\maketitlehookd{\vspace{-2ex}}

\setlength\parindent{0pt}

\newcommand{\imply}[1]{
  \-\hspace{1em}$\implies$ \parbox[t]{11.2cm}{#1}
}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\begin{document}
\maketitle

\tableofcontents

\section{Essential Patterns}
\hrule\vspace{5ex}

\subsection{Backtracking}

Backtracking is a strategy for solving \textit{combinatorial search} problems, where the goal is to find groupings or arrangements of elements that satisfy certain conditions. Backtracking searches the space of all possible solution states, and this space can be represented by a tree known as a \textit{state-space tree} or \textit{potential search tree}. The nodes of this tree are \textit{partial candidates}. Each partial candidate is the parent of the candidates that differ from it by a single \textit{extension step}, and the leaves of the tree are the partial candidates that cannot be extended further. The tree is searched in depth-first order. \\

An efficient backtracking algorithm will prune the subtree rooted at a partial candidate that cannot be extended to a valid solution (i.e. it will not bother exploring such a subtree). At each node $c$, it will check whether $c$ can be extended to a valid solution. If it cannot, it will prune the subtree rooted at $c$ and backtrack from $c$. Otherwise, it will check whether $c$ itself is a valid solution, and then it will explore the subtree rooted at $c$. \\

A backtracking algorithm that does no pruning is equivalent to a \textit{brute-force} or \textit{exhaustive} search of the state-space. The \textit{actual search tree} of a backtracking algorithm is the pruned version of its potential search tree. \\

In general, backtracking solutions can be implemented with the following template, where \texttt{reject(c)} returns whether \texttt{c} can be extended to a valid solution, \texttt{accept(c)} returns whether \texttt{c} is a valid solution, and \texttt{get\_steps(c)} returns a list of the elements that can be appended to \texttt{c} to extend it to a child partial candidate: \medskip

\begin{python}
result = []
def backtrack(c):
    if reject(c):
        return
    if accept(c):
        result.append(c[:])

    for step in get_steps(c):
        c.append(step)
        backtrack(c)
        c.pop()

backtrack([])
\end{python} \medskip

Note that this template assumes that a valid solution can be extended to another valid solution. If this is not the case, the \texttt{accept} conditional should return after appending \texttt{c} to \texttt{result} (i.e. when valid solutions must be leaves of the potential or actual search tree). \\

Sometimes a \texttt{reject} conditional is unnecessary because an exhaustive search is required. For example, for a backtracking algorithm that finds all possible combinations of \texttt{k} numbers in the range \texttt{[1, n]}, valid solutions must be leaves and every leaf is a valid solution: \medskip

\begin{python}
def combine(n: int, k: int) -> list[list[int]]:
    result = []

    def backtrack(state, start):
        if len(state) == k:
            result.append(state[:])
            return
       
        need = k - len(state)
        remain = n - start + 1
        available = remain - need

        for step in range(start, start + available + 1):
            state.append(step)
            start += 1
            backtrack(state, start)  # Take
            state.pop()  # Not take

    backtrack([], 1)
    return result
\end{python}

\subsection{Dynamic Programming}

(combinatorial optimization) \\
\texttt{@functools.cache}

\subsubsection{Would it help to rephrase the problem in order to more easily define its subproblems?}

\begin{center}
\fbox{\begin{minipage}{28em}
Given an integer array, return the length of the longest strictly increasing subsequence (LIS). \smallskip\\
\-\hspace{1em}$\equiv$\hspace{0.5em} Return the length of the LIS of an array \texttt{a} of length $n$. \\
\-\hspace{1em}$\equiv$\hspace{0.5em} Return the length of the LIS of \texttt{a[0:n]}. \bigskip\\
The LIS of \texttt{a} must have some first element. If this is the $i$th element, then the LIS of \texttt{a} is equal to the LIS of \texttt{a[i:]}, where \texttt{a[i]} is the first element of the sequence. \medskip\\
Let \texttt{dp[i]} be the length of the LIS of \texttt{a[i:]}, where \texttt{a[i]} is the first element of the sequence. Return \texttt{max(dp)}.
\end{minipage}} \bigskip
\end{center}

\subsubsection{Is the problem a variation of the \textit{knapsack problem}?}

\begin{center}
\fbox{\begin{minipage}{28em}
Given a set of $N$ items, each with a weight and a value, determine which items to include in a knapsack such that the total weight is less than or equal to the knapsack's capacity $W$ and the total value is as large as possible. Return the total value.
\end{minipage}} \medskip
\end{center}

In the \textbf{0-1 knapsack problem}, each item can be taken once or not at all. \medskip
\begin{center}
\begin{varwidth}{\linewidth}
\begin{verbatim}
K(n, w) = max(val[n - 1] + K(n - 1, w - wt[n - 1]),
                           K(n - 1, w))
\end{verbatim}
\end{varwidth}
\end{center}

In the \textbf{unbounded knapsack problem}, each item can be taken an arbitrary number of times. \medskip
\begin{center}
\begin{varwidth}{\linewidth}
\begin{verbatim}
K(n, w) = max(val[n - 1] + K(n, w - wt[n - 1]),
                           K(n - 1, w))
\end{verbatim}
\end{varwidth}
\end{center}

Below are three 0-1 knapsack implementations: memoization, 2D tabulation, and 1D tabulation. \medskip

\begin{python}
def knapsack_memo(wt: list[int], val: list[int], W: int) -> int:
    N = len(wt)
    dp = [[-1 for _ in range(W + 1)] for _ in range(N + 1)]

    def knapsack(n, w):
        if n == 0 or w == 0:
            return 0

        if dp[n][w] == -1:
            if wt[n - 1] > w:
                dp[n][w] = knapsack(n - 1, w)
            else:
                dp[n][w] = max(
                    val[n - 1] + knapsack(n - 1, w - wt[n - 1]),
                    knapsack(n - 1, w)
                )

        return dp[n][w]

    return knapsack(N, W)
\end{python}

\begin{python}
def knapsack_tab_2d(wt: list[int], val: list[int], W: int) -> int:
    N = len(wt)
    dp = [[0 for _ in range(W + 1)] for _ in range(N + 1)]

    for n in range(1, N + 1):
        for w in range(1, W + 1):
            if wt[n - 1] > w:
                dp[n][w] = dp[n - 1][w]
            else:
                dp[n][w] = max(
                    val[n - 1] + dp[n - 1][w - wt[n - 1]],
                    dp[n - 1][w]
                )

    return dp[N][W]
\end{python}

\begin{python}
def knapsack_tab_1d(wt: list[int], val: list[int], W: int) -> int:
    N = len(wt)
    dp = [0] * (W + 1)

    for n in range(1, N + 1):
        for w in range(W, wt[n - 1] - 1, -1):
            dp[w] = max(val[n - 1] + dp[w - wt[n - 1]], dp[w])

    return dp[W]
\end{python}

\subsection{Sets}

\subsubsection{Do you need to model the partitioning of a set? That is, given a set of items, do you need to group the items into subsets?}

You should use a disjoint-set (union-find) forest (see Appendix B.2).

\subsection{Arrays}

\subsubsection{Would it help to know the sum of elements for any subarray in O(n) time?}

Computing the \textbf{prefix sum} of an array \texttt{a} will give you the sum of elements for subarrays \texttt{[a[:i] for i in range(1, len(a))]}. By subtracting elements of the prefix sum from each other, you can get the sum of elements for any subarray. That is, \texttt{sum(a[x:y]) = sum(a[:y]) - sum(a[:x])} for $x < y$. \\

\subsubsection{Would it help to know if two multisets are permutations of each other?}

\textit{Fundamental theorem of arithmetic: every integer greater than 1 can be represented uniquely as a product of prime numbers.} \medskip

You can design a hash function that uses \textbf{prime factorization} to map multisets to unique integers. For example, you can map all permutations (anagrams) of a string to a unique integer like so: \medskip

\begin{python}
def compute_hash(s: str) -> int:
    alphabet_primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,
                       31, 37, 41, 43, 47, 53, 59, 61, 67,
                       71, 73, 79, 83, 89, 97, 101]
    h = 1
    for ch in s:
        h *= alphabet_primes[ord(ch) - ord('a')]
    return h
\end{python} \bigskip

\subsubsection{Do you need to find the previous/next lesser/greater element for each element in a given array?}

You should use a \textbf{monotonic stack}. There are four types of monotonic stack: \textit{increasing}, \textit{decreasing}, \textit{non-increasing}, and \textit{non-decreasing}. These stacks are used to find next greater elements, previous greater elements, next lesser elements, and previous lesser elements, and all of these problems can be solved using the template code below: \medskip

\begin{python}
def _find_indicies(arr, op, r):
    stack = []
    result = [-1] * len(arr)
    for i in r:
        while stack and op(arr[i], arr[stack[-1]]):
            result[stack.pop()] = i
        stack.append(i)
    return result
\end{python} \medskip

\begin{center}
\begin{tabular}{|l|l|c|c|}
  \hline
  Problem Type & Stack Type & Loop Conditional & Direction \\
  \hline
  Next Greater & Non-Increasing & \texttt{curr > top} & $\rightarrow$ \\
  Previous Greater & Non-Increasing & \texttt{curr > top} & $\leftarrow$ \\
  Next Lesser & Non-Decreasing & \texttt{curr < top} & $\rightarrow$ \\
  Previous Lesser & Non-Decreasing & \texttt{curr < top} & $\leftarrow$ \\
  \hline
\end{tabular}
\end{center} \bigskip

If it is preferable to loop from left to right while looking for previous greater elements or previous lesser elements, the template code below can be used instead: \medskip

\begin{python}
def _find_indicies2(arr, op):
    stack = []
    result = [-1] * len(arr)
    for i in range(len(arr)):
        while stack and op(arr[i], arr[stack[-1]]):
            stack.pop()
        if stack:
            result[i] = stack[-1]
        stack.append(i)
    return result
\end{python} \medskip

\begin{center}
\begin{tabular}{|l|l|c|c|}
  \hline
  Problem Type & Stack Type & Loop Conditional & Direction \\
  \hline
  Previous Greater & Decreasing & \texttt{curr >= top} & $\rightarrow$ \\
  Previous Lesser & Increasing & \texttt{curr <= top} & $\rightarrow$ \\
  \hline
\end{tabular}
\end{center} \bigskip

\subsubsection{Do you need to point to the middle node of a linked list?}

\begin{python}
def find_mid(head):
    slow, fast = head, head.next
    while fast and fast.next:
        slow = slow.next
        fast = fast.next.next
    return slow
\end{python}

\subsubsection{Does the problem involve a changing median?}

Consider sorting the array and turning the left half into a max-heap and/or the right half into a min-heap.

\subsection{Graphs}
\subsubsection{Do you need to detect a cycle in an undirected graph?}

You should use a disjoint-set (union-find) forest (see Appendix B.2). The vertices are the elements of the subsets, and a union of subsets corresponds to an edge between vertices/components. If calling \texttt{union(x, y)} does not change the structure of the forest, then you know that \texttt{x} and \texttt{y} belong to the same component and that an edge between them would produce a cycle.

\subsubsection{Do you need to connect verticies together without cycles while minimizing total edge-weight?}

Such a set of edges is known as the \textbf{minimum spanning tree} (MST) of a graph. You should use \textit{Kruskal's algorithm} (see Appendix A.1) or \textit{Prim's algorithm} (see Appendix A.2). The former is slightly preferred for sparse graphs, the latter for dense graphs.

\subsubsection{Do you need to traverse every edge of a graph exactly once?}

Such a sequence of edges is known as an \textbf{Eulerian trail} and finding such a trail was the goal of the famous \textit{Seven Bridges of K\"onigsberg} problem. Similarly, an \textbf{Eulerian cycle} is an Eulerian trail that starts and ends at the same vertex. \\

For connected graphs:
\begin{itemize}
  \item An undirected graph:
  \begin{itemize}
    \item Has an Eulerian cycle iff every vertex has even degree.
    \item Has an Eulerian trail that is not a cycle iff exactly two verticies have odd degree (the start and end verticies).
  \end{itemize}
  \item A directed graph:
  \begin{itemize}
    \item Has an Eulerian cycle iff every vertex has equal in-degree and out-degree.
    \item Has an Eulerian trail that is not a cycle iff at most one vertex has \texttt{out-degree - in-degree = 1} (the start vertex) and at most one vertex has \texttt{in-degree - out-degree = 1} (the end vertex). \medskip
  \end{itemize}
\end{itemize}

To find an Eulerian cycle in a graph, you should use Hierholzer's algorithm: \medskip

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{$G = (V, E)$}
  \KwResult{$C$ (a sequence of edges that represents an Eulerian cycle)}
  $C \longleftarrow$\hspace{0.5mm}\texttt{[]}\;
  Follow a trail of unused edges starting at $s \in V$ until it returns to $s$, appending each edge to $C$\hspace{58mm}\texttt{/* N1 */}\;
  \While{$\exists\ u \in V \mid u$ is in the trail and has unused adjacent edges}{
      $D \longleftarrow$\hspace{0.5mm}\texttt{[]}\;
      Follow a trail of unused edges starting at $u$ until it returns to $u$, appending each edge to $D$\;
      Insert $D$ into $C$ before some edge leaving $u$\;
  }
  \KwRet{$C$}\;
  \caption{Hierholzer's Algorithm\hspace{5mm}\texttt{/* see A.3 for code */}}
\end{algorithm} \medskip

\texttt{N1:} The trail will not get stuck and fail to return to $s$ because all $v \in V$ have even degree $\implies$ when the trail enters another vertex $w$, $w$ must have an unused edge leaving $w$.

\subsection{Searching}

\subsubsection{Binary Search}
To find a given target (for duplicate targets, return index of first target found in the search):
\begin{python}
def binary_search(nums: list[int], target: int) -> int:
    left, right = 0, len(nums) - 1
    while left <= right:
        mid = (left + right) // 2
        if nums[mid] < target:
            left = mid + 1
        elif nums[mid] > target:
            right = mid - 1
        else:
            return mid
    return -1
\end{python}
\bigskip

To find the leftmost duplicate target (if target does not exist, return number of elements less than target (rank of target)):
\begin{python}
def binary_search_leftmost(nums: list[int], target: int) -> int:
    left, right = 0, len(nums)
    while left < right:
        mid = (left + right) // 2
        if nums[mid] < target:
            left = mid + 1
        else:
            right = mid
    return left
\end{python}
\texttt{bisect.bisect\_left(nums, target)} \\

To find the rightmost duplicate target (if target does not exist, \texttt{(n - right)} is the number of elements greater than target):
\begin{python}
def binary_search_rightmost(nums: list[int], target: int) -> int:
    left, right = 0, len(nums)
    while left < right:
        mid = (left + right) // 2
        if nums[mid] > target:
            right = mid
        else:
            left = mid + 1
    return right - 1
\end{python}
\texttt{bisect.bisect\_right(nums, target) - 1} \\
\texttt{bisect.bisect(nums, target) - 1}

\subsection{Sorting}

\subsubsection{Do you need to sort items according to a custom scheme?}
\begin{itemize}
  \item \texttt{functools.cmp\_to\_key} 
  \item Create class and define dunder methods \texttt{\_\_lt\_\_}, \texttt{\_\_gt\_\_}, \texttt{\_\_le\_\_}, \texttt{\_\_ge\_\_}, \texttt{\_\_eq\_\_}, \texttt{\_\_ne\_\_}
\end{itemize}
\bigskip

\subsubsection{Do you need to schedule tasks based on their dependencies?}

You can apply \textbf{topological sorting} to a directed graph. This will produce a linear ordering of the vertices such that for every directed edge \textit{uv} from vertex \textit{u} to vertex \textit{v}, \textit{u} comes before \textit{v}. However, if the graph has cycles, such an ordering does not exist.\medskip\\

There are two main topological sorting algorithms: \textit{Kahn's algorithm} (BFS) and \textit{cycle detection via DFS}. The former cannot visit cycles and detects them by checking for unvisited nodes after traversal. The latter detects cycles by entering the first one it finds and completing a loop.\\

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{$G = (V, E)$}
  \KwResult{$L$ (list of $v \in V$ in topological order)}
  $L \longleftarrow$\hspace{0.5mm}\texttt{[]}\;
  $S \longleftarrow \{v \in V \mid v \text{ has no incoming edges}\}$\;
  \While{S is not empty}{
    remove a node $n$ from $S$\;
    append $n$ to $L$\;
    \ForEach{node $m$ with an edge $e$ from $n$ to $m$}{
      remove $e$ from $E$\;
      \If{$m$ has no incoming edges}{
        add $m$ to $S$\;
      }
    }
  }
  \eIf{$E$ is empty}{
    \KwRet{$L$}
  }{
    \KwRet{error}\tcc*[r]{the graph has a cycle}
  }
  \caption{Kahn's Algorithm\hspace{13mm}\texttt{/* see A.4 for code */}}
\end{algorithm}
\bigskip

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{FnVisit}{visit}
  \KwData{$G = (V, E)$}
  \KwResult{$L$ (list of $v \in V$ in topological order)}
  $L \longleftarrow$\hspace{0.5mm}\texttt{[]}\;
  \Fn{\FnVisit{node $n$}}{
    \If{$n$ has a permanent mark}{
      \KwRet{}
    }
    \If{$n$ has a temporary mark}{
      \Stop\tcc*[r]{the graph has a cycle}
    }
    mark $n$ with a temporary mark\;
    \ForEach{node $m$ with an edge from $n$ to $m$}{
      visit($m$)\;
    }
    remove temporary mark from $n$\;
    mark $n$ with a permanent mark\;
    prepend $n$ to $L$\;
  }
  \While{$\exists$ nodes without a permanent mark}{
    select an unmarked node $n$\;
    visit($n$)\;
  }
  \KwRet{$L$}\;
  \caption{DFS Topological Sort\hspace{8mm}\texttt{/* see A.5 for code */}}
\end{algorithm}

\subsection{Bit Manipulation}

\begin{itemize}
  \item $x = y \implies x \oplus y = 0$
  \item Need to flip $x$? $x \oplus 1$
  \item Two's complement...
\end{itemize}

\section{Useful Python Constructs}
\hrule\vspace{5ex}

Do you need to...
\begin{itemize}
  \item Count items in a collection? \smallskip\\
    \imply{\texttt{collections.Counter} creates a dictionary of the form \texttt{\{element: count\}}}
  \item Return a default value for keys not found in a dictionary? \smallskip\\
    \imply{\texttt{collections.defaultdict}}
  \item Get the ASCII value of a character? \smallskip\\
    \imply{\texttt{ord(ch)}}
  \item Reverse a list? \smallskip\\
    \imply{The fastest method is the ``Martian smiley" \texttt{[::-1]}}
  \item Determine if a string is a palindrome? \smallskip\\
    \imply{\texttt{s == s[::-1]}}
\end{itemize}

\texttt{itertools.combinations}, \texttt{itertools.permutations} \\
\texttt{re} (regex) \\
\texttt{enumerate} $\rightarrow$ \texttt{count, value} \\
\texttt{map}, \texttt{filter}, \texttt{reduce}, \texttt{zip} \\
deep copy, shallow copy \\
\texttt{id} \\
\texttt{contextlib.suppress}?

\section{Unorganized}
\hrule\vspace{5ex}

\begin{itemize}
  \item DFS $\rightarrow$ stack (recursion) $\rightarrow$ LIFO
  \item BFS $\rightarrow$ queue (iteration) $\rightarrow$ FIFO
  \item Online tests: have a Python scratchpad open, spam the ``Run Tests" button (EAFP > LBYL)
  \item Number of subarrays of array of size $n$ (also, the sum of an arithmetic progression, $\sum_{i=0}^{n}i$): $\frac{n(n+1)}{2}$
  \item Python is pass-by-assignment
  \begin{itemize}
    \item Immutable objects are pass-by-value
    \item Mutable objects are pass-by-reference
    \item You can rebind the variable in the inner scope, but the outer scope will remain unchanged
  \end{itemize}
  \item Some DP notes
  \begin{itemize}
    \item Optimal substructure $\implies$ divide and conquer
    \item Optimal substructure + greedy choice $\implies$ greedy
    \item Optimal substructure + overlapping subproblems \\
          \-\hspace{1em}$\implies$ dynamic programming
  \end{itemize}
\end{itemize}

\section{To Do}
\hrule\vspace{5ex}

\subsection{Essential Topics}
\begin{itemize}
  \item Intervals?
  \item Floyd's Tortoise and Hare Cycle Detection Algorithm
  \item Helper method recursion (parameter or nonlocal)
  \item Kadane's algorithm (maximum subarray)
  \item Dijkstra's algorithm (shortest path in weighted graph)
  \item Sweep line algorithm (convex hull)
  \item Sliding window
  \item LRU Cache (hash map + DLL, OrderedDict)
  \item Monotonic queue/deque (max/min element in sliding window) % https://www.youtube.com/watch?v=J1rJ3IcwhKI
  \item Tries
\end{itemize}

\subsection{Stretch Topics}
\begin{itemize}
  \item Rabin-Karp (string-searching, uses a rolling hash to make approximate comparisons between substring hash and target hash, makes exact comparison if hashes match)
  \item Sieve of Eratosthenes (find all prime numbers up to a given integer)
  \item Segment trees and interval trees
\end{itemize}

\newpage
\appendix

\section{Algorithms}

\subsection{Minimum Spanning Trees - Kruskal's Algorithm}

Kruskal's algorithm is a greedy algorithm that, in each step, adds to the MST the lightest edge that will not form a cycle. It uses a disjoint-set forest to detect whether adding an edge will form a cycle. \\

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{$G = (V, E)$ with edge weights $w(e)$ for $e\in E$}
  \KwResult{$T$ (a set of edges that represents an MST)}
  $T \longleftarrow$\hspace{0.5mm}\texttt{[]}\;
  $D \longleftarrow$\hspace{0.5mm} disjoint-set forest of $V$\;
  Sort $E$ by weight, increasing\;
  \ForEach{$(u, v)\in E$}{
    \If{$u$ and $v$ do not belong to the same disjoint set in $D$}{
        Add $(u, v)$ to $T$\;
        Union $u$ and $v$ in $D$\;
    }
  }
  \KwRet{$T$}\;
  \caption{Kruskal's Algorithm}
\end{algorithm} \medskip

See Appendix B.2 for an implementation of a disjoint-set forest with weighted union and collapsing find. Given an edge list representing a graph and a weight function, Kruskal's algorithm can be implemented as follows: \medskip

\begin{python}
def kruskal(edges, weight):
    verticies = set()
    for u, v in edges:
        verticies.add(u)
        verticies.add(v)
    verticies = list(verticies)
    n = len(verticies)

    vertex_to_index = {}
    for i in range(n):
        vertex_to_index[verticies[i]] = i

    edges.sort(key=weight)
    uf = DisjointSet(n)

    mst = []
    for u, v in edges:
        i, j = vertex_to_index[u], vertex_to_index[v]
        if uf.union(i, j):
            mst.append((u, v))

    return mst
\end{python}

Note that the values in the disjoint-set forest correspond to the indicies of the vertex list, but the disjoint sets in the forest represent groupings of the verticies themselves. Kruskal's algorithm can also find the minimum spanning forest of a disconnected graph.

\subsection{Minimum Spanning Trees - Prim's Algorithm}
Prim's algorithm is a greedy algorithm that, in each step, adds to the MST the lightest edge that will connect a vertex in the MST to a vertex not in the MST. It can be described at a high level by the following three steps:

\begin{enumerate}
  \item Initialize a tree with a single vertex, chosen arbitrarily from the graph.
  \item Grow the tree by one edge; of the edges that connect the tree to verticies not yet in the tree, find the minimum-weight edge and transfer it to the tree.
  \item Repeat step 2 until all verticies are in the tree.
\end{enumerate}

To find the minimum-weight edge from a tree vertex to a non-tree vertex, it would help to have a heap or priority queue. The heap could contain every non-tree vertex (initially, every vertex) and prioritize them based on the minimum cost to connect them to the tree (weight of lightest edge connecting to tree). Cost would be finite for adjacent verticies, infinite otherwise. As long as we keep track of which tree vertex provides that min-cost connection for each non-tree vertex, we will know which edges to add to the MST. \\

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{$G = (V, E)$ with edge weights $w(e)$ for $e\in E$}
  \KwResult{$T$ (a set of edges that represents an MST)}
  \ForEach{$v \in V$}{
    $C(v) \coloneqq \infty$ (cost of cheapest connection to $v$)\;
    $P(v) \coloneqq$ \texttt{null} (vertex that provides cheapest connection to $v$)\;
  }
  $H \longleftarrow$\hspace{0.5mm} priority queue of $V$, using $C$ as priorities\;
  \hspace{13mm}(contains the verticies not yet in the tree)\;
  \While{$H$ is not empty}{
    Pop $u$ from $H$ (the cheapest vertex to connect to the tree)
    \ForEach{$e = (u, v)$ where $v$ is not yet in the tree}{
      \If{$e$ is the cheapest path to $v$ found so far ($w(e) < C(v)$)}{
        Increase the priority of $v$ in $H$\hspace{16mm} ($C(v)\coloneqq w(e)$)\;
        Let $v$ connect to $u$ when $v$ is popped\hspace{4mm} ($P(v)\coloneqq u$)\;
      }
    }
  }
    $T \longleftarrow$\hspace{0.5mm} \{$(P(v), v)\;\forall\,v\in V\mid P(v)$ is not \texttt{null}\}\;
  \KwRet{$T$}\;
    \caption{Prim's Algorithm (heap with \textit{decrease-key})}
\end{algorithm} \medskip

Note this line from the above pseudocode:
\begin{center}
Increase the priority of $v$ in $H$\hspace{4mm} ($C(v)\coloneqq w(e)$)
\end{center}
Increasing the priority of an arbitrary entry in a priority queue requires a heap with a \textit{decrease-key} operation. The Python \texttt{heapq} module does not provide this operation, so the pseudocode above must be implemented with a custom heap. With the priority queue implementation given in Appendix B.1, the algorithm can be written as follows: \medskip

\begin{python}
def prim(adj_list, weight):
    prev = {}
    pq = PriorityQueue([[inf, v] for v in adj_list])
    while pq:
        u = pq.pop()
        for v in adj_list[u]:
            if v in pq and (w := weight((u, v))) < pq.get_priority(v):
                pq.decrease_key(v, priority=w)
                prev[v] = u

    return [(prev[v], v) for v in prev]
\end{python}

However, the code required to implement a heap with \textit{decrease-key} is cumbersome, and the algorithm can be written without modifying entries in the priority queue. Instead of initializing the heap with all of the verticies, verticies can be added when they become adjacent to the tree. And when costs to connect are lowered, duplicate entries can be inserted with higher priority. The priority queue will contain stale data, but if we keep track of which verticies have been added to the tree, then duplicate entries of lower priority can be discarded when popped. \\

\begin{algorithm}[H]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{$G = (V, E)$ with edge weights $w(e)$ for $e\in E$}
  \KwResult{$T$ (a set of edges that represents an MST)}
  \ForEach{$v \in V$}{
    $C(v) \coloneqq \infty$ (cost of cheapest connection to $v$)\;
    $P(v) \coloneqq$ \texttt{null} (vertex that provides cheapest connection to $v$)\;
  }
  $H \longleftarrow$\hspace{0.5mm} priority queue containing arbitrary $v\in V$, highest priority\;
  \hspace{13mm}(contains the verticies adjacent to but not yet in the tree and\;
  \hspace{13mm}also possibly stale duplicates of verticies in the tree)\;
  \While{there are verticies that are not in the tree}{
    Pop $u$ from $H$ until $u$ is not in the tree (discard duplicates)\;
    Add $u$ to the tree\;
    \ForEach{$e = (u, v)$ where $v$ is not yet in the tree}{
      Increase the priority of $v$, insert $v$ into $H$\hspace{4mm} ($C(v)\coloneqq w(e)$)\;
      Let $v$ connect to $u$ when $v$ is popped\hspace{12mm} ($P(v)\coloneqq u$)\;
    }
  }
  $T \longleftarrow$\hspace{0.5mm} \{$(P(v), v)\;\forall\,v\in V\mid P(v)$ is not \texttt{null}\}\;
  \KwRet{$T$}\;
    \caption{Prim's Algorithm (heap with duplicate entries)}
\end{algorithm} \medskip

Because the heap no longer requires \textit{decrease-key}, the \texttt{heapq} module is sufficient: \medskip

\begin{python}
def prim(adj_list, weight):
    cost = {v: inf for v in adj_list}
    prev = {}
    tree = set()
    pq = [(0, next(iter(adj_list)))]

    while len(tree) < len(adj_list):
        while (u := heapq.heappop(pq)[1]) in tree: pass
        tree.add(u)
        for v in adj_list[u]:
            if v not in tree and (w := weight((u, v))) < cost[v]:
                heapq.heappush(pq, (w, v))
                cost[v] = w
                prev[v] = u
        
    return [(prev[v], v) for v in prev]
\end{python}

Note that the first implementation of Prim's algorithm will find the minimum spanning forest for a disconnected graph. The second implementation will only find the minimum spanning tree of the connected component that it starts in, but the function could be called on every connected component in order to find the minimum spanning forest.

\subsection{Eulerian Cycle Detection - Hierholzer's Algorithm}

Below are two functions that find the Eulerian cycle in a directed graph where such a cycle is assumed to exist. The former returns a list of verticies, the latter returns a list of edges. These functions can also be used to find an Eulerian trail, provided that \texttt{s} is set to the vertex with \texttt{out-degree - in-degree = 1}, if such a vertex exists. \medskip

\begin{python}
from collections import deque

# Returns the vertex sequence of the Eulerian cycle
def hierholzer_vertices(graph: dict[str, list[str]], s: str) -> list[str]:
    cycle = deque()
    def dfs(v):
        while graph[v]:
            dfs(graph[v].pop())
        cycle.appendleft(v)

    dfs(s)
    return cycle


# Returns the Eulerian cycle
def hierholzer_edges(graph: dict[str, set[str]], s: str) -> list[str]:
    cycle = deque()
    def dfs(src, dst):
        while graph[dst]:
            dfs(dst, graph[dst].pop())
        cycle.appendleft([src, dst])
        while graph[src]:
            dfs(src, graph[src].pop())

    dfs(s, graph[s].pop())
    return cycle
\end{python}

\subsection{Topological Sorting - Kahn's Algorithm}
\begin{python}
def find_order_bfs(adj_list: list[list[int]],
                   in_degrees: list[int]) -> list[int]:
    # 1. Create list of start nodes
    queue = deque()
    for n, d in enumerate(in_degrees):
        if d == 0:
            queue.append(n)

    topo_order = []
    while queue:
        # 2. Add a start node n to the topological ordering
        n = queue.popleft()
        topo_order.append(n)

        # 3. Remove edges from n to its neighbors
        #    Add neighbors of in-degree 0 to start node list
        for m in adj_list[n]:
            in_degrees[m] -= 1
            if in_degrees[m] == 0:
                queue.append(m)

    return topo_order if len(topo_order) == len(adj_list) else []
\end{python}

\subsection{Topological Sorting - DFS Cycle Detection}
\begin{python}
def find_order_dfs(adj_list: list[list[int]]) -> list[int]:
    visited = set()
    dfs_tree = set()
    topo_order = []

    def has_cycle(n):
        if n in visited:  # path already explored
            return False
        if n in dfs_tree:  # cycle detected
            return True

        dfs_tree.add(n)
        for m in adj_list[n]:
            if has_cycle(m):
                return True

        dfs_tree.remove(n)
        visited.add(n)
        topo_order.append(n)
        return False

    for n in range(len(adj_list)):
        if has_cycle(n):
            return []
    return topo_order
\end{python}

\section{Data Structures}

\subsection{Heaps and Priority Queues}

A \textbf{heap} is a tree-based data structure that satisfies the heap property:
\begin{itemize}
  \item For a min-heap, every parent is less than or equal to its children.
  \item For a max-heap, every parent is greater than or equal to its children.
\end{itemize}

Heaps are useful when you need direct access to the smallest or largest element in a mutable or dynamic collection. \\

Heaps are usually implemented with arrays. For a binary heap, the node stored at index $0$ is the root, and a node stored at index $i$ has children at indicies $2i + 1$ and $2i + 2$ and a parent at $\floor{(i - 1)/2}$. \\

A min-heap has three essential operations:
\begin{center}
\begin{tabular}{|l|c|l|}
  \hline
  \textbf{Operation} & \textbf{Time} & \textbf{Python Function} \\
  \hline
  Heapify & $O(n)$ & \texttt{heapq.heapify(heap)} \\
  Insert & $O(\log n)$ & \texttt{heapq.heappush(heap, item)} \\
  Extract-min & $O(\log n)$ & \texttt{heapq.heappop(heap)} \\
  \hline
\end{tabular}
\end{center}

\begin{itemize}
  \item \textit{Heapify}: for all non-leaf nodes, from the last ($i = \floor{n/2} - 1$) to the root, sift the node down (for a max of one swap)
  \item \textit{Insert}: append the new item to the array, sift it up
  \item \textit{Extract-min}: swap the root $r$ with the last element of the array $x$, pop $r$ from the end of the array, sift $x$ down, return $r$
\end{itemize}

The Python \texttt{heapq} module provides two more heap operations that improve efficiency when you push-then-pop or pop-then-push. The amount of sift operations required is reduced from two to one:
\begin{center}
\begin{tabular}{|l|c|l|}
  \hline
  \textbf{Operation} & \textbf{Time} & \textbf{Python Function} \\
  \hline
  Push-pop & $O(\log n)$ & \texttt{heapq.heappushpop(heap, item)} \\
  Replace & $O(\log n)$ & \texttt{heapq.heapreplace(heap, item)} \\
  \hline
\end{tabular}
\end{center}

\begin{itemize}
  \item \textit{Push-pop}: if the new item $x$ is less than the root $r$, return $x$; else, save $r$, overwrite the first element of the array with $x$, sift $x$ down, return $r$
  \item \textit{Replace}: append the new item to the array, call \textit{extract-min}
\end{itemize}

Heaps are often used to implement \textbf{priority queues}. A priority queue is an abstract data type similar to a queue or stack. Each element in a priority queue has an associated \textit{priority}, and elements with high priority are dequeued before elements with low priority. A priority queue has two essential operations: \textit{insert with priority} and \textit{extract element with highest priority}. \\

There are other operations that a priority queue could have. For example, if a priority queue is being used to organize a set of tasks, it might be useful to increase the priority of a task as circumstances change. To do this, the priority queue (or, technically, the underlying heap) would need a \textit{decrease-key} operation (assuming that smaller keys correspond to higher priorities). \\

The \texttt{heapq} module does not provide this operation; see the code below for a priority queue implementation that has \textit{decrease-key}:
\medskip
\begin{python}
class PriorityQueue:
    def __init__(self, items=[]):
        self.pq = items
        self.index = {t: i for (i, [_, t]) in enumerate(items)}
        self._heapify()

    def __len__(self):
        return len(self.pq)

    def __contains__(self, task):
        return task in self.index

    def insert(self, task, priority=0):
        self.pq.append([priority, task])
        self._sift_up(len(self.pq) - 1)

    def pop(self):
        if len(self.pq):
            self.pq[0], self.pq[-1] = self.pq[-1], self.pq[0]
            self.index[self.pq[0][1]] = 0
            _, task = self.pq.pop()
            del self.index[task]
            self._sift_down(0)
            return task
        raise KeyError('Pop from empty priority queue')

    def decrease_key(self, task, priority=0):
        if self.get_priority(task) <= priority:
            raise ValueError('New priority is not less than old priority')
        c = self.index[task]
        self.pq[c][0] = priority
        self._sift_up(c)

    def get_priority(self, task):
        if task not in self.index:
            raise KeyError('Task not in priority queue')
        return self.pq[self.index[task]][0]

    def _heapify(self):
        for p in range(len(self.pq) // 2 - 1, -1, -1):
            self._sift_down(p)

    def _sift_up(self, c):
        p = (c - 1) // 2
        if c > 0 and self.pq[c] < self.pq[p]:
            self._swap(c, p)
            self._sift_up(p)

    def _sift_down(self, p):
        n = len(self.pq)
        cl, cr = 2 * p + 1, 2 * p + 2
        if cl >= n:
            return
        if cr >= n:
            cr = cl
        c = cl if self.pq[cl] < self.pq[cr] else cr

        if self.pq[c] < self.pq[p]:
            self._swap(c, p)
            self._sift_down(c)

    def _swap(self, i, j):
        self.index[self.pq[i][1]] = j
        self.index[self.pq[j][1]] = i
        self.pq[i], self.pq[j] = self.pq[j], self.pq[i]
\end{python}

\subsection{Disjoint-Set (Union-Find) Forest}
A disjoint-set forest models the partitioning of a set. Initially, each element of the set belongs to a subset where it is the only member. Two subsets can be united into a single subset that contains the elements of each. The union of a set with itself is itself. \\

These subsets are represented as trees in the structure, and the structure has two operations on these trees: \texttt{union(x, y)} and \texttt{find(x)}, where \texttt{x} and \texttt{y} are elements of the set. When \texttt{union(x, y)} is called, the subset that \texttt{x} belongs to is united with the subset that \texttt{y} belongs to. Structurally, the root of one tree becomes the child of the other tree's root. If \texttt{x} and \texttt{y} belong to the same set, the structure does not change. When \texttt{find(x)} is called, the root of the tree that \texttt{x} belongs to is returned. This is the ``representative member" of the set, a kind of ``name" for the set. \\

The following class arbitrarily chooses \texttt{x} to be the parent of \texttt{y} upon their union. Here, the parent of a root is itself, but $0$ would also be a fine choice.
\medskip
\begin{python}
class DisjointSet:
    def __init__(self, n):
        self.parent = list(range(n))
        # self.parent = [0] * n

    def union(self, x, y):
        self.parent[self.find(y)] = self.find(x)

    def find(self, x):
        return x if x == self.parent[x] else self.find(self.parent[x])
        # while self.parent[x]:
        #     x = self.parent[x]
        # return x
\end{python}
\bigskip

The following class implements two enhancements known as \textit{weighted union} and \textit{collapsing find}. The parent of a root is now a negative number whose absolute value corresponds to the tree's \textit{weight} or \textit{rank}. When \texttt{union(x, y)} is called, where \texttt{x}'s tree has greater weight than \texttt{y}'s tree, the weight of \texttt{y}'s tree will be added to the weight of \texttt{x}'s tree, and the root of \texttt{y}'s tree will point to the root of \texttt{x}'s tree. This ensures that the united tree is more balanced. \texttt{find(x)} now sets the parent of any node on the path from \texttt{x} to the representative member of the tree to the representative member. Initially, \texttt{find(x)} is $O(\log n)$, but subsequent calls are $O(1)$.
\medskip
\begin{python}
class DisjointSet:
    def __init__(self, n):
        self.parent = [-1] * n

    def union(self, x, y):
        rx, ry = self.find(x), self.find(y)
        if rx == ry:
            return False
        elif self.parent[rx] < self.parent[ry]:
            self.parent[rx] += self.parent[ry]
            self.parent[ry] = rx
        else:
            self.parent[ry] += self.parent[rx]
            self.parent[rx] = ry
        return True

    def find(self, x):
        if self.parent[x] < 0:
            return x
        self.parent[x] = self.find(self.parent[x])
        return self.parent[x]
\end{python}
\bigskip

Note that the elements of a disjoint set could be represented with something other than integers (so \texttt{self.parent} would be a dictionary), but weighted union would not be able to be implemented as it is above.

\section{Glossary}

\subsection{Graph Theory}
\begin{itemize}
  \item \textit{Walk}: a sequence of edges which joins a sequence of vertices
  \item \textit{Trail}: a walk in which all edges are distinct
  \item \textit{Cycle}: a trail that begins and ends at the same vertex
  \item \textit{Path}: a trail in which all verticies are distinct
\end{itemize}
\end{document}

% \begin{tikzpicture}
% \matrix (m) [matrix of nodes,
%              nodes={draw, minimum size=8mm, anchor=center},
%              nodes in empty cells, minimum height=1cm,
%              row 1/.style={nodes={draw=none}},]
% {
%   \texttt{0} & \texttt{1} & \texttt{2} & \texttt{3} & \texttt{4} & \texttt{5} & \texttt{6} & \texttt{7} & \texttt{8} \\
%   2 & 4 & 4 & 5 &   &   &   &  &\\
% };
% \draw[semithick] (-3.66, -0.9) -- ++(0, -0.1) -- ++(0.81, 0) -- ++(0, 0.1);
% \draw[semithick] (-3.66, -1.1) -- ++(0, -0.1) -- ++(1.62, 0) -- ++(0, 0.1);
% \draw[semithick] (-3.66, -1.3) -- ++(0, -0.1) -- ++(2.43, 0) -- ++(0, 0.1);
% \draw[semithick] (-3.66, -1.5) -- ++(0, -0.1) -- ++(3.24, 0) -- ++(0, 0.1);
% \end{tikzpicture}

